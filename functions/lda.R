## This file contains functions related to running LDA using Mallet and working with the 
## output.


if (!require("mallet"))
  install.packages("mallet")

if(!require("wordcloud"))
  install.packages("wordcloud")

library(mallet)
library(wordcloud)


#' Train an LDA model using Mallet.
#'
#' @param documents A data frame with two attributes
#'         $id: the ids for the documents
#'        $text: the raw text of the document
#' @param num.topics The number of topis to be trained.
#' @param stoplist A path to the stopword list to use
#' @param token.regexp A regular expression for splitting the text into tokens
#'
#' @return The trianed topic model as a list. This
#'   documents: the documents used to train this topic model
#'   vocabulary: A vector of all tokens found in the supplied documents (after any
#'         transformations). Note that the order of this vector matters since the
#'         The vocabulary is the starting point for identify the various words in
#'         topic vectors and other structures.
#'   wordFreq: Word frequency counts for each term in the vocabulary. This is a
#'         data frame in which each row containes the index of the token, the observed
#'         token (stemmed, if stemming is performed), the total number of times the
#'         token appears and the number of documents in which the token appears
#'   model: The trained topic model.
#'   topics: The matrix of topics as a K x N matrix where K is the number of topics
#'         and N is the number of words in the corpus. Each row corresponds to a topic,
#'         while each column contains the likelihood that a particular word will be
#'         generated by this model. Word order is given by the vocabulary vector.
#'   docAssigments: The topic porportions for each document. A matrix with D rows
#'         adn K columns.
#'   K: The number of topics for this model
#'   N: The number of terms in the vocabulary of this model
#'   D: The number of documents used to train this model
trainSimpleLDAModel <- function(documents,
                                num.topics,
                                stoplist = "data/stoplist.csv",
                                token.regexp="\\p{L}[\\p{L}\\p{P}]+\\p{L}")
{
  mallet.instances <- mallet.import(unlist(documents$id),
                                    unlist(documents$text),
                                    stoplist,
                                    FALSE,
                                    token.regexp=token.regexp)
  topic.model <- MalletLDA(num.topics=num.topics)
  topic.model$loadDocuments(mallet.instances)
  vocabulary <- topic.model$getVocabulary()
  word.freqs <- mallet.word.freqs(topic.model)
  topic.model$train(1000)

  # construct result 
  # Note that all of this information can be easily retrieved via the trained 
  # topic model (as we do here). The motivation for loading this information 
  # in an R list rather than simply referencing the data structure provided by
  # Mallet is that this will allow us to write the data in a file format of 
  # our choice. This will allow us to work with data produced by different 
  # systems and to share it with others (as opposed to the Java-specific 
  # serialization forms used by Mallet)
  result <- list()
  result$model <- topic.model
  result$documents <- documents
  result$vocabulary <- vocabulary
  result$wordFreq <- word.freqs

  result$topics <- mallet.topic.words(topic.model, smoothed=TRUE, normalized=TRUE)
  result$docAssignments <- mallet.doc.topics(topic.model, normalized=T, smoothed=F)
  colnames(result$topics) <- vocabulary

  result$K <- nrow(result$topics)
  result$N <- ncol(result$topics)
  result$D <- nrow(result$documents)
  
  result$getTopic <- function(k) {
    return (getTopic(result, k))
  }
  
  result$getDocument <- function(id) {
    ix <- which(documents$id == id)
    return (documents$text[ix])
  }

  return(result)
}

#' Creates
#' 
#' @param lda.model The topic model, constructed with trainSimpleLDAModel
#' @param k The index of the topic to be returned, in the range [1, lda.model$K]
#' 
#' @return The requrested topic as a list. Contains the following members
#'   label:    A label for this topic, initially generic 'Topic k'.
#'   model:    The supplied model, for reference
#'   k:        The index of the returned topic
#'   getWords: A function to retrieve the top words of the document
#'   getDocs:  A function to retrieve the top documents associated with this topic
getTopic <- function(lda.model, k)
{
  
  getModel <- function() 
  {
    return (lda.model)
  }
  
  getWords <- function(ct = lda.model$D, sorted=T)
  {
    terms <- lda.model$topic[k, ]
    if (sorted)
      terms <- terms[order(terms, decreasing=T)][1:ct]
    
    return (terms)
  }
  
  getDocs <- function(ct = lda.model$D) 
  {
    docs <- lda.model$docAssignments[,k]
    names(docs) <- lda.model$documents$id
    
    docs <- docs[order(docs, decreasing=T)][1:ct]
    
    return (docs)
  }
  
  topic <- list()
  topic$label <- paste("Topic", k)
  topic$k <- k
  topic$getModel <- getModel
  topic$getWords <- getWords
  topic$getDocs <- getDocs
  
  return (topic)
}

#' Compute the difference between two topics from the same model using 
#' the Kullback-Leibler distance. 
#' 
#' The standard KL divergence is not a valid metric since it depends on 
#' the order of the vectors being compared (that is KL(a, b) != KL(b, a)).
#' It can be though of as the relative information gain of using a instead
#' of b. As a distance metric, however, it is necessary for the order of 
#' the operands to be independent. To acheive this, this function simply
#' computes KL(a, b) + KL(b, a). 
topic.diff.kl <- function(topic.a, topic.b, same.model=F) 
{
  terms.a <- topic.a$getWords(sorted=F)
  terms.b <- topic.b$getWords(sorted=F)
  
  if (!same.model)
  {
    min.val <- min(union(topic.a$getWords(), topic.b$getWords()))
    if (min.val <= 0)
      min.val <- 10e-7  # make sure this isn't zero
                   
    vocab <- union(names(terms.a), names(terms.b))
    
    # set missing values to zero, note that we could also handle missing
    # values by ignoring the impact of those terms, using only the shared vocab
    terms.a <- terms.a[vocab]
    terms.a[which(is.na(terms.a))] <- min.val;
    
    terms.b <- terms.b[vocab]
    terms.b[which(is.na(terms.b))] <- min.val;
  }
  

  kl.div.a = sum(terms.a * log(terms.a / terms.b))
  kl.div.b = sum(terms.b * log(terms.b / terms.a))
  
  return (kl.div.a + kl.div.b)
}

#' Compute the similarity matrix of topics within a model using KL divergence 
#' of the topics term PDF. 
#' 
#' @param lda.model The trained topic model.
#' @return A KxK matrix with 
topic.similarity.kl <- function(lda.model)
{
  # HACK: need to use function application instead of loops
  K <- lda.model$K
  similarity <- matrix(nrow = K, ncol = K)
  for (ix.a in 1:(K - 2))
  { 
    
    topic.a <- lda.model$getTopic(ix.a)
# TODO requires testing
#    similarity[ix.a, ix.a+1:K] <- sapply(ix.a+1:K, function(ix.b) {
#      topic.b <- lda.model$getTopic(ix.b)
#      return (topic.diff.kl(topic.a, topic.b))
#    })
    for (ix.b in (ix.a+1):K)
    {
      topic.b <- lda.model$getTopic(ix.b)
      
      kl.diff <- topic.diff.kl(topic.a, topic.b)
      similarity[ix.a, ix.b] <- kl.diff[1]
      similarity[ix.b, ix.a] <- kl.diff[1]
    }
    
    # set the self similarity for this topic
    similarity[ix.a, ix.a] <- 0
  }
  
  # add column and row names with topic index
  colnames(similarity) <- 1:K
  rownames(similarity) <- 1:K
  
  return (similarity)
}

#' Returns a list of the topics, ranked by thier similarity to a given topic.
#' 
#' @param similarity.m The similarity matrix
#' @param k The topic to rank the results in comparison to 
topic.similarity.findsimilar <- function(similarity.m, k) 
{
  # the similarity metric for topics relative to K in increasing order 
  similar.k <- similarity.m[k, order(similarity.m[k,])]
  # Use topic indices as names for vector elements
  names(similar.k) <- order(similarity.m[k,])
  
  return (similar.k)
}

#' Plot all topics within a topic model as wordclouds and print to a file. 
#' 
#' @param lda.model The LDA model whose topics should be plotted.
#' @param output    The directory to write the wordcloud image files to. Defaults to "plots".
#' @param prefix    A string to prefix on the generated plots
#' @param num.words The number of words to display in the wordcloud. Defaults to 100.
#' @param verbose   Print status updates as the wordclouds are being generated.
plotTopicWc <- function(topic, output="plots", prefix="Topic-", num.words=100, verbose=F)
{
  K <- topic$getModel()$K
  k <- topic$k
  top.words <- topic$getWords(num.words);
  plot.fname <- paste(output, "/", prefix, k, ".png", sep="")
  
  if (verbose)
    print(paste("Generating wordcloud for topic ", k, "of", K, ":", plot.fname))
    
  png(plot.fname, width=12, height=8, units="in", res=300)
  wordcloud(names(top.words),
            top.words,
            scale=c(4, .2),
            rot.per=0.1,
            random.order=FALSE,
            colors=brewer.pal(8, "Dark2"))
  dev.off()
}

#' Plot all topics within a topic model as wordclouds and print to a file. 
#' 
#' @param lda.model The LDA model whose topics should be plotted.
#' @param output    The directory to write the wordcloud image files to. Defaults to "plots".
#' @param prefix    A string to prefix on the generated plots
#' @param num.words The number of words to display in the wordcloud. Defaults to 100.
#' @param verbose   Print status updates as the wordclouds are being generated.
#'
plotTopicWordcloud <- function(lda.model, output="plots", prefix="Topic-", num.words=100, verbose=F)
{
  if (verbose)
    print(paste("Generating wordclouds for", lda.model$K, "topics."))

  for (topic.ix in 1:lda.model$K)
  {
    plotTopicWc(getTopic(lda.model, topic.ix), output, prefix, num.words, verbose)
  }

  if (verbose)
    print("Finished generating wordclouds.");
}
